---
title: "Pilot Library DBR Descriptive Statistics"
output: pdf_document
---
```{r echo=FALSE, cache=FALSE}
## numbers >= 10^5 will be denoted in scientific notation
## and rounded to 2 digits
options(scipen = 0, digits = 2)
```
# Background
  
The preliminary QC for the pilot library is documented in ~/home/Dropbox/ColoState/Projects/Outputs

The raw pilot library data are in ~/home/Downloads/Sample1_ACTTGA_L008_R1_001.fastq.gz and ~/home/Downloads/Sample1_ACTTGA_L008_R2_001.fastq.gz

The counts of degenerate base region sequences were obtained with the following bash one-liner:

```{r eval=FALSE}
$ zcat Sample1_ACTTGA_L008_R2_001.fastq.gz | sed -n '2~4p' | 
cut -c 1-8 | sort | uniq -c | sort -nr -k 1 > sample1_pilot_degeneracycheck.txt
```

# Goal

Develop a likelihood-based means of detecting PCR duplicates using the expected distribution of DBR sequences.

# Visualize DBR sequence recovery

## Load the data and look at raw counts

```{r}
dbr.pilot <- read.table('~/Downloads/sample1_pilot_degeneracycheck.txt')
head(dbr.pilot)
tail(dbr.pilot)
```

The most common sequences are very guanine-rich. This indicates that the distribution of DBR sequences in the library is not uniform, and that the skew of the distribution should be accounted for in detecting and removing duplicates.

## Barplot of sequence counts

The count of sequences (ordered by frequency) confirms that it's definitely not a uniform distribution.

```{r}
barplot(dbr.pilot[,1])
```

## Histogram of sequence counts

The distribution of counts is bimodal, with modes near 1 and near 3500.

```{r}
hist(dbr.pilot[,1], main='', xlab='Sequence Count')
```

The full histogram is difficult to visualize because of the high frequency at the first mode. With some trial and error we find the distribution can be broken into two parts right around sequences observed 100 times and plotted on different y-axis scales for easier visualization.

```{r}
no.small <- dbr.pilot[dbr.pilot[,1]>=50,]
small.only <- dbr.pilot[dbr.pilot[,1]<50,]
par(mfrow=c(1,2))
hist(small.only[,1], main='', xlab='Sequence Count')
hist(no.small[,1], main='', xlab='Sequence Count')
```

# Fit a sensible distribution to the data

The smaller mode is probably almost entirely sequencing errors. However, a distribution that fully describes the data would be nice.

## Full distributions I considered (thought experiments rather than formal fitting)

- **Multinomial** is the most flexible, but the dimensionality would be entirely too high to be tractable.

- **Categorical** distributions would also be really easy to use as a descriptor of the distribution, but do not lend well to developing a likelihood function.

Neither of these are really satisfying. Do we really need to describe the full distribution?

## Distributions considered for describing only the "non-error" section of the data around the second mode

- **Poisson** won't work because the full data are bimodal. The curve around second mode (the part probably not sequencing error) is not by itself Poisson; variance is much larger than the mean.

```{r}
mu <- mean(no.small[,1])
v <- var(no.small[,1])
v/mu
```

- **Negative binomial** can serve as an over-dispersed Poisson for the curve around the second mode. But the parameters r (number of failures) and k (number of successes) don't have a clear biological interpretation and I'd have to find some other way to fit the distribution. So for now, the negative binomial is set aside.

- **Normal** distribution seems like a possibility -- the shape around the second mode could be convincingly normal.

```{r}
# The Normal PMF
norm <- function(mu, sigma, x){
  y <- (1/(sigma*sqrt(2*pi)))*exp(-((x-mu)^2)/(2*(sigma^2)))
  return(y)
}

# The descriptive stats for the non-error data subset
mu <- mean(no.small[,1])
sigma <- sd(no.small[,1])

# Reasonable x values
x <- seq(min(no.small[,1]), max(no.small[,1]))

# The predicted and observed distributions
y <- norm(mu, sigma, x)
hist(no.small[,1], freq=F, main='', xlab='Sequence Count')
lines(seq(min(no.small[,1]), max(no.small[,1])), y)
```

The normal distribution seems promising! Let's check the assumption that we really can remove the infrequently observed DBRs...

# DBR Error Rate

We have `r sum(dbr.pilot[,1])` reads, and the expected Illumina per-base error rate is approximately 0.1% (per http://www.molecularecologist.com/next-gen-table-3c-2014/). Is it possible that the `r sum(small.only[,1])` DBRs observed 100 or fewer times could be the result of sequencing error?

```{r}
# define some constants
err.rate = 0.001
num.reads = sum(dbr.pilot[,1])
read.len = 125

# number of bases in a single DBR
dbr.len = 8

# probability of no incorrect bases in a single DBR
dbr.correct = (1-err.rate)^dbr.len

# probability of at least one incorrect base in a single DBR
dbr.err = 1-dbr.correct

# expected number of DBRs with at least one incorrect base
expect.dbr.err = dbr.err*num.reads

# observed number of DBRs with counts fewer than 100 (putative errors)
num.infrequent = sum(small.only[,1])

# do we observe fewer infrequent (erroneous) DBRs than we expect?
num.infrequent < expect.dbr.err
```

```{r echo=FALSE}
ede = round(expect.dbr.err)
ni = round(num.infrequent)
```

We expect $`r ede`$ reads to contain a DBR with at least 1 incorrect base. But we observe $`r round(num.infrequent, digits=2)`$ infrequent DBRs, far more than we would expect if they were all attributable to sequencing error.

# Mixture model?

It looks like a poisson + gaussian, or a couple negative binomial distributions, could fully describe the data.

## *mixtools* Package

This package only does mixtures of the same distribution. The normal distribution is by far the easiest to implement, and definitely not what we want.

Poisson + normal and negative binomial are sadly not available.

```{r}
library(mixtools)
mmdl <- normalmixEM(dbr.pilot[,1])
summary(mmdl)
plot(mmdl, which=2)

```

## *flexmix* Package

```{r}
library(flexmix)


```

## *gamlss* Package
```{r, echo=FALSE}
library(gamlss)
gl.test <- histDist(dbr.pilot[,1], "NBII")

library(gamlss.mx)
glmx.test <- gamlssMX(dbr.pilot[,1]~1, family='NBI', data=dbr.pilot, K=2)
```

# Try next: Conway-Maxwell-Poisson Distribution, a poisson mixture model that allows for bimodality.

---------------------------------------------------
\pagebreak

# How does quality filtering influence the distribution of the DBR sequences recovered?

Ideally we filter error OUT of the DBRs in the sequence data before using them to infer the true distribution of DBRs in the adapter pool. Error may change the shape of the DBR distribution... does it? 

The script

```
quality_dbr_sensitivity.py
```
Performs a sensitivity analysis on the quality filters employed by FASTX-Toolkit's *fastq\_qualit\y_filter* function, and then using *sed* to extract the DBR sequences and save them to a file.

The quality for each base is given as an ASCII-33 score (inferred from looking at the characters used in the quality line and the values reported in this table: http://drive5.com/usearch/manual/quality\_score.html). The *fastq\_quality\_filter* takes two important arguments, one for the minimum base score and one for the fraction of bases that have to have at least that median. I have required that at least 50% of bases have the minimum score, meaning that the median score will be no less than the minimum. ASCII-33 scores range from 1-41.

Load the data:
```{r}
setwd('~/Desktop/CSU_ChronicWasting/PilotAnalysis/QualDBR_Sensitivity/')
qual.files=list.files(getwd())
qual.counts=grep('.txt', qual.files, value=T)
```

## Sensitivity analysis with reciprocal transformed data

This is bad for a couple of reasons, but it stays in the file because it was useful for thinking about the problem.

Reciprocally transforming count data will give a weird distribution -- 1/1 = 1; 1/2 = 0.5. There will be a big gap in the distribution (between 0.5 and 1) where there cannot be any data.

I also plotted after removing singletons. However, we WANT DBRs with a low frequency of occurrence. Especially if they are high quality. Singletons should not be removed at this phase (removing singleton DNA sequences once DBRs are removed, however, is sensible).

```{r}
#png(file='~/Desktop/CSU_ChronicWasting/PilotAnalysis/DBR_Distr_Qual_Sensitivity.png', width=30, height=40, units='cm', res=300)
par(mfrow=c(4,2))
for(file in qual.counts){
  dbr.counts=read.table(file)
  hist(1/dbr.counts[,1], xlim=c(0,1), main=paste('Minimum Median Quality = ', substr(file, 1, 2)), xlab='1/DBR Counts')
  
  no.singletons=dbr.counts[dbr.counts[,1]>1,]
  hist(1/no.singletons[,1], xlim=c(0,1), main=paste('Minimum Median Quality = ', substr(file, 1, 2), '(No Singletons)'), xlab='1/DBR Counts')
}
#dev.off()

```

## Sensitivity analysis on DBR proportions with logit transformation

Expressing DBR frequency as a fraction of all reads is arguably better for downstream analysis because we will ultimately gauge DBR occurrence in assembled loci against DBR occurrence in the original (quality filtered) data. If we used raw counts, DBR occurrence in the assembled loci would almost always be less than the expected overall count.

Another benefit of using the fraction of reads with a given DBR is that expressing count frequency as a proportion allows us to use the logit transformation. The distribution of DBR fractions is highly, highly right skewed. After performing the logit transformation we can better see the shape of the distribution and confirm that it is (1) unimodal and (2) likely to fit a 

```{r}
png(file='~/Desktop/CSU_ChronicWasting/DBR_distributions.png', height=40, width=30, units='cm', res=300)
par(mfrow=c(4,3))
for(file in qual.counts){
  dbr.counts=read.table(file)
  counts.fraction=dbr.counts[,1]/sum(dbr.counts[,1])
  no.singletons=dbr.counts[dbr.counts[,1]>1,]
  no.single.counts.frac=no.singletons[,1]/sum(no.singletons[,1])
  counts.logit=log((counts.fraction)/(1-counts.fraction))
  hist(counts.fraction, main=paste('Minimum Median Quality = ', substr(file, 1, 2)), xlab='Fraction of Reads with DBR', freq=FALSE)
  hist(no.single.counts.frac, main=paste('Minimum Median Quality = ', substr(file, 1, 2)), xlab='Fraction of Reads with DBR (no singletons)', freq=FALSE)
  hist(counts.logit-min(counts.logit), main=paste('Minimum Median Quality = ', substr(file, 1, 2)), xlab='translated logit(Fraction of Reads with DBR)')
}
dev.off()
```